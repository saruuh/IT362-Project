{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c3c584",
   "metadata": {},
   "source": [
    "# IT362: Principles of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac449a",
   "metadata": {},
   "source": [
    "## Phase 1: Data Collection Research and Assessment  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca1870d",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "The **Natural Disaster Prediction Model: PREDINA** aims to analyze and predict the impacts of natural disasters based on historical data, with a focus on enhancing disaster preparedness and response strategies. The main research question guiding this project is: *How can historical data on natural disasters inform future predictions and improve community resilience?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee887f",
   "metadata": {},
   "source": [
    "### 2. Importing Libraries\n",
    "In this section, we will import the necessary libraries for our analysis and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42176333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install openpyxl\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f01a4",
   "metadata": {},
   "source": [
    "### 3. Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28710ab0",
   "metadata": {},
   "source": [
    "- #### **EM-DAT: The International Disaster Database**\n",
    "The EM-DAT Public Table is a global disaster database maintained by CRED, tracking natural and technological disasters. It includes data on fatalities, affected populations, and economic damages, and is used for research and disaster management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6134a5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          DisNo. Historic Classification Key Disaster Group Disaster Subgroup  \\\n",
      "0  1999-9388-DJI       No    nat-cli-dro-dro        Natural    Climatological   \n",
      "1  1999-9388-SDN       No    nat-cli-dro-dro        Natural    Climatological   \n",
      "2  1999-9388-SOM       No    nat-cli-dro-dro        Natural    Climatological   \n",
      "3  2000-0001-AGO       No    tec-tra-roa-roa  Technological         Transport   \n",
      "4  2000-0002-AGO       No    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "\n",
      "  Disaster Type Disaster Subtype External IDs Event Name  ISO  ...  \\\n",
      "0       Drought          Drought          NaN        NaN  DJI  ...   \n",
      "1       Drought          Drought          NaN        NaN  SDN  ...   \n",
      "2       Drought          Drought          NaN        NaN  SOM  ...   \n",
      "3          Road             Road          NaN        NaN  AGO  ...   \n",
      "4         Flood   Riverine flood          NaN        NaN  AGO  ...   \n",
      "\n",
      "  Reconstruction Costs ('000 US$) Reconstruction Costs, Adjusted ('000 US$)  \\\n",
      "0                             NaN                                       NaN   \n",
      "1                             NaN                                       NaN   \n",
      "2                             NaN                                       NaN   \n",
      "3                             NaN                                       NaN   \n",
      "4                             NaN                                       NaN   \n",
      "\n",
      "  Insured Damage ('000 US$) Insured Damage, Adjusted ('000 US$)  \\\n",
      "0                       NaN                                 NaN   \n",
      "1                       NaN                                 NaN   \n",
      "2                       NaN                                 NaN   \n",
      "3                       NaN                                 NaN   \n",
      "4                       NaN                                 NaN   \n",
      "\n",
      "  Total Damage ('000 US$) Total Damage, Adjusted ('000 US$)        CPI  \\\n",
      "0                     NaN                               NaN  58.111474   \n",
      "1                     NaN                               NaN  56.514291   \n",
      "2                     NaN                               NaN  56.514291   \n",
      "3                     NaN                               NaN  56.514291   \n",
      "4                 10000.0                           17695.0  56.514291   \n",
      "\n",
      "                                         Admin Units  Entry Date  Last Update  \n",
      "0  [{\"adm1_code\":1093,\"adm1_name\":\"Ali Sabieh\"},{...  2006-03-01   2023-09-25  \n",
      "1  [{\"adm1_code\":2757,\"adm1_name\":\"Northern Darfu...  2006-03-08   2023-09-25  \n",
      "2  [{\"adm1_code\":2691,\"adm1_name\":\"Bay\"},{\"adm1_c...  2006-03-08   2023-09-25  \n",
      "3                                                NaN  2004-10-27   2023-09-25  \n",
      "4  [{\"adm2_code\":4214,\"adm2_name\":\"Baia Farta\"},{...  2005-02-03   2023-09-25  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_excel(\"Datasets/emdat.xlsx\")\n",
    "print(df1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74994bc",
   "metadata": {},
   "source": [
    "- #### **Kaggle Dataset : ALL NATURAL DISASTERS 1900-2021 / EOSDIS**\n",
    "This dataset, hosted on Kaggle, provides a record of natural disasters worldwide from 1900 to 2021, sourced from NASA's Earth Observing System Data and Information System (EOSDIS). It includes details such as disaster type, location, dates, and impacts (e.g., fatalities, affected populations, and economic damages), making it useful for analyzing historical disaster trends and impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0309ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year   Seq Glide Disaster Group Disaster Subgroup      Disaster Type  \\\n",
      "0  1900  9002   NaN        Natural    Climatological            Drought   \n",
      "1  1900  9001   NaN        Natural    Climatological            Drought   \n",
      "2  1902    12   NaN        Natural       Geophysical         Earthquake   \n",
      "3  1902     3   NaN        Natural       Geophysical  Volcanic activity   \n",
      "4  1902    10   NaN        Natural       Geophysical  Volcanic activity   \n",
      "\n",
      "  Disaster Subtype Disaster Subsubtype   Event Name     Country  ...  \\\n",
      "0          Drought                 NaN          NaN  Cabo Verde  ...   \n",
      "1          Drought                 NaN          NaN       India  ...   \n",
      "2  Ground movement                 NaN          NaN   Guatemala  ...   \n",
      "3         Ash fall                 NaN  Santa Maria   Guatemala  ...   \n",
      "4         Ash fall                 NaN  Santa Maria   Guatemala  ...   \n",
      "\n",
      "  No Affected No Homeless Total Affected Insured Damages ('000 US$)  \\\n",
      "0         NaN         NaN            NaN                        NaN   \n",
      "1         NaN         NaN            NaN                        NaN   \n",
      "2         NaN         NaN            NaN                        NaN   \n",
      "3         NaN         NaN            NaN                        NaN   \n",
      "4         NaN         NaN            NaN                        NaN   \n",
      "\n",
      "  Total Damages ('000 US$)       CPI Adm Level Admin1 Code Admin2 Code  \\\n",
      "0                      NaN  3.221647       NaN         NaN         NaN   \n",
      "1                      NaN  3.221647       NaN         NaN         NaN   \n",
      "2                  25000.0  3.350513       NaN         NaN         NaN   \n",
      "3                      NaN  3.350513       NaN         NaN         NaN   \n",
      "4                      NaN  3.350513       NaN         NaN         NaN   \n",
      "\n",
      "  Geo Locations  \n",
      "0           NaN  \n",
      "1           NaN  \n",
      "2           NaN  \n",
      "3           NaN  \n",
      "4           NaN  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"Datasets/EOSDIS.csv\")\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb9d1d",
   "metadata": {},
   "source": [
    "- ### **Global Disaster Alert and Coordination System (GDACS)**\n",
    "The GDACS API provides real-time alerts on natural disasters such as earthquakes, tsunamis, and storms, offering data on disaster type, location, magnitude, and impact. It is useful for monitoring, coordinating disaster response efforts, and supplementing data for building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f8c757",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'end_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/IT362-Project/Phase1.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-bassoon-4jw74xwq7pv9cqv5v/workspaces/IT362-Project/Phase1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m start_date \u001b[39m=\u001b[39m properties\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mfromdate\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m1900-01-01\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-bassoon-4jw74xwq7pv9cqv5v/workspaces/IT362-Project/Phase1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m start_year, start_month, start_day \u001b[39m=\u001b[39m start_date\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Borganic-bassoon-4jw74xwq7pv9cqv5v/workspaces/IT362-Project/Phase1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m end_year, end_month, end_day \u001b[39m=\u001b[39m end_date\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-bassoon-4jw74xwq7pv9cqv5v/workspaces/IT362-Project/Phase1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Determine magnitude based on alert level\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Borganic-bassoon-4jw74xwq7pv9cqv5v/workspaces/IT362-Project/Phase1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m alert_level \u001b[39m=\u001b[39m properties\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39malertlevel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGreen\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'end_date' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# API URL\n",
    "api_url = \"https://www.gdacs.org/gdacsapi/api/events/geteventlist/EVENTS4APP\"\n",
    "\n",
    "# API Parameters (Modify dates as needed)\n",
    "params = {\n",
    "    \"from\": \"2010-01-01\",  # Modify this\n",
    "    \"to\": \"2024-02-06\",  # Modify this\n",
    "    \"source\": \"DFO\",\n",
    "    \"alertlevel\": \"RED\",\n",
    "    \"datatype\": \"4DAYS\",\n",
    "    \"type\": \"json\"\n",
    "}\n",
    "\n",
    "# Send API request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()  # Convert response to JSON\n",
    "    features = data.get(\"features\", [])  # Extract list of disaster events\n",
    "\n",
    "    # CSV Output File\n",
    "    csv_filename = \"gdacs_disasters.csv\"\n",
    "\n",
    "    # Define CSV Headers\n",
    "    headers = [\n",
    "        \"Event Name\", \"Country\", \"ISO\", \"Disaster Group\", \"Disaster Subgroup\",\n",
    "        \"Disaster Type\", \"Disaster Subtype\", \"Latitude\", \"Longitude\",\n",
    "        \"Start Year\", \"Start Month\", \"Start Day\", \"End Year\", \"End Month\", \"End Day\",\n",
    "        \"Magnitude\", \"Affected People\"\n",
    "    ]\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)  # Write header row\n",
    "\n",
    "        for feature in features:\n",
    "            properties = feature.get(\"properties\", {})\n",
    "            geometry = feature.get(\"geometry\", {})\n",
    "\n",
    "            # Extracting required fields\n",
    "            event_name = properties.get(\"name\", \"Unknown\")\n",
    "            country = properties.get(\"country\", \"Unknown\")\n",
    "            iso = properties.get(\"iso3\", \"Unknown\")\n",
    "            disaster_group = properties.get(\"Class\", \"Unknown\")  # Adjust as needed\n",
    "            disaster_subgroup = \"Unknown\"  # No direct field found in API\n",
    "            disaster_type = properties.get(\"eventtype\", \"Unknown\")\n",
    "            disaster_subtype = \"Unknown\"  # No direct field found in API\n",
    "\n",
    "            # Extract latitude & longitude\n",
    "            latitude, longitude = None, None\n",
    "            if geometry.get(\"type\") == \"Point\":\n",
    "                coordinates = geometry.get(\"coordinates\", [])\n",
    "                if len(coordinates) >= 2:\n",
    "                    longitude, latitude = coordinates[0], coordinates[1]\n",
    "\n",
    "            # Extract start and end dates\n",
    "            start_date = properties.get(\"fromdate\", \"1900-01-01\").split(\"T\")[0]\n",
    "            \n",
    "\n",
    "            start_year, start_month, start_day = start_date.split(\"-\")\n",
    "            end_year, end_month, end_day = end_date.split(\"-\")\n",
    "\n",
    "            # Determine magnitude based on alert level\n",
    "            alert_level = properties.get(\"alertlevel\", \"Green\")\n",
    "            if alert_level == \"RED\":\n",
    "                magnitude = \">4\"\n",
    "            elif alert_level == \"ORANGE\":\n",
    "                magnitude = \">2\"\n",
    "            else:\n",
    "                magnitude = \"<=2\"\n",
    "\n",
    "            # Affected people\n",
    "            affected_people = properties.get(\"totalaffected\", \"N/A\")\n",
    "\n",
    "            # Write row to CSV\n",
    "            writer.writerow([\n",
    "                event_name, country, iso, disaster_group, disaster_subgroup,\n",
    "                disaster_type, disaster_subtype, latitude, longitude,\n",
    "                start_year, start_month, start_day, end_year, end_month, end_day,\n",
    "                magnitude, affected_people\n",
    "            ])\n",
    "\n",
    "    print(f\"Data successfully saved to {csv_filename}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15cf90",
   "metadata": {},
   "source": [
    "Convert Data Types to Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976255a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [\"Latitude\", \"Longitude\", \"Start Year\", \"Start Month\", \"Start Day\", \n",
    "                   \"End Year\", \"End Month\", \"End Day\", \"Total Deaths\", \"No. Injured\", \n",
    "                   \"No. Affected\", \"No. Homeless\", \"Total Affected\", \"Total Damages ('000 US$)\", \"CPI\", \"Magnitude\"]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in df1.columns:\n",
    "        df1[col] = pd.to_numeric(df1[col], errors='coerce')  # Convert to numeric, set errors to NaN\n",
    "    if col in df2.columns:\n",
    "        df2[col] = pd.to_numeric(df2[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d8b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = [\"Year\", \"Disaster Group\", \"Disaster Subgroup\", \"Disaster Type\", \"ISO\", \"Magnitude Scale\"]\n",
    "\n",
    "for col in string_columns:\n",
    "    if col in df1.columns:\n",
    "        df1[col] = df1[col].astype(str).str.strip()  # Ensure string format\n",
    "    if col in df2.columns:\n",
    "        df2[col] = df2[col].astype(str).str.strip()\n",
    "\n",
    "print(\"Updated df1 column types:\\n\", df1.dtypes)\n",
    "print(\"Updated df2 column types:\\n\", df2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de665b7d",
   "metadata": {},
   "source": [
    "### 4. Data Intergration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f6bcfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisNo.\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisNo.\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[:\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m      3\u001b[0m df1\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisNo.\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m df1\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Damage, Adjusted (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m000 US$)\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Damages (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m000 US$\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1['DisNo.'] = df1['DisNo.'].astype(str).str[:4]\n",
    "\n",
    "df1.rename(columns={'DisNo.': 'Year'}, inplace=True)\n",
    "df1.rename(columns={\"Total Damage, Adjusted ('000 US$)\": \"Total Damages ('000 US$')\"}, inplace=True)\n",
    "\n",
    "df2.rename(columns={'Dis Mag Value': 'Magnitude'}, inplace=True)\n",
    "df2.rename(columns={'Dis Mag Scale': 'Magnitude Scale'}, inplace=True)\n",
    "df2.rename(columns={'No Injured': 'No. Injured'}, inplace=True)\n",
    "df2.rename(columns={'No Affected': 'No Affected'}, inplace=True)\n",
    "df2.rename(columns={'No Homeless': 'No. Homeless'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859bf58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 columns and types:\n",
      " Year                                          object\n",
      "Historic                                      object\n",
      "Classification Key                            object\n",
      "Disaster Group                                object\n",
      "Disaster Subgroup                             object\n",
      "Disaster Type                                 object\n",
      "Disaster Subtype                              object\n",
      "External IDs                                  object\n",
      "Event Name                                    object\n",
      "ISO                                           object\n",
      "Country                                       object\n",
      "Subregion                                     object\n",
      "Region                                        object\n",
      "Location                                      object\n",
      "Origin                                        object\n",
      "Associated Types                              object\n",
      "OFDA/BHA Response                             object\n",
      "Appeal                                        object\n",
      "Declaration                                   object\n",
      "AID Contribution ('000 US$)                  float64\n",
      "Magnitude                                    float64\n",
      "Magnitude Scale                               object\n",
      "Latitude                                     float64\n",
      "Longitude                                    float64\n",
      "River Basin                                   object\n",
      "Start Year                                     int64\n",
      "Start Month                                  float64\n",
      "Start Day                                    float64\n",
      "End Year                                       int64\n",
      "End Month                                    float64\n",
      "End Day                                      float64\n",
      "Total Deaths                                 float64\n",
      "No. Injured                                  float64\n",
      "No. Affected                                 float64\n",
      "No. Homeless                                 float64\n",
      "Total Affected                               float64\n",
      "Reconstruction Costs ('000 US$)              float64\n",
      "Reconstruction Costs, Adjusted ('000 US$)    float64\n",
      "Insured Damage ('000 US$)                    float64\n",
      "Insured Damage, Adjusted ('000 US$)          float64\n",
      "Total Damage ('000 US$)                      float64\n",
      "Total Damages ('000 US$)                     float64\n",
      "CPI                                          float64\n",
      "Admin Units                                   object\n",
      "Entry Date                                    object\n",
      "Last Update                                   object\n",
      "dtype: object\n",
      "df2 columns and types:\n",
      " Year                           object\n",
      "Seq                             int64\n",
      "Glide                          object\n",
      "Disaster Group                 object\n",
      "Disaster Subgroup              object\n",
      "Disaster Type                  object\n",
      "Disaster Subtype               object\n",
      "Disaster Subsubtype            object\n",
      "Event Name                     object\n",
      "Country                        object\n",
      "ISO                            object\n",
      "Region                         object\n",
      "Continent                      object\n",
      "Location                       object\n",
      "Origin                         object\n",
      "Associated Dis                 object\n",
      "Associated Dis2                object\n",
      "OFDA Response                  object\n",
      "Appeal                         object\n",
      "Declaration                    object\n",
      "Aid Contribution              float64\n",
      "Magnitude                     float64\n",
      "Magnitude Scale                object\n",
      "Latitude                       object\n",
      "Longitude                      object\n",
      "Local Time                     object\n",
      "River Basin                    object\n",
      "Start Year                      int64\n",
      "Start Month                   float64\n",
      "Start Day                     float64\n",
      "End Year                        int64\n",
      "End Month                     float64\n",
      "End Day                       float64\n",
      "Total Deaths                  float64\n",
      "No. Injured                   float64\n",
      "No. Affected                  float64\n",
      "No. Homeless                  float64\n",
      "Total Affected                float64\n",
      "Insured Damages ('000 US$)    float64\n",
      "Total Damages ('000 US$)      float64\n",
      "CPI                           float64\n",
      "Adm Level                      object\n",
      "Admin1 Code                    object\n",
      "Admin2 Code                    object\n",
      "Geo Locations                  object\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on float64 and object columns for key 'Latitude'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing columns in df2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, missing_cols_df2)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Merge DataFrames\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Drop duplicates and null values\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m merged_df\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:1508\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1504\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1505\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1506\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1507\u001b[0m     ):\n\u001b[0;32m-> 1508\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on float64 and object columns for key 'Latitude'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print column names and types to debug\n",
    "print(\"df1 columns and types:\\n\", df1.dtypes)\n",
    "print(\"df2 columns and types:\\n\", df2.dtypes)\n",
    "\n",
    "# Rename 'DisNo.' to 'Year' in df1 if it exists\n",
    "if 'DisNo.' in df1.columns:\n",
    "    df1.rename(columns={'DisNo.': 'Year'}, inplace=True)\n",
    "\n",
    "# Convert 'Year' to int if possible, or keep it as a string\n",
    "df1['Year'] = df1['Year'].astype(str).str[:4]  # Ensure it's a 4-digit string\n",
    "df2['Year'] = df2['Year'].astype(str)  # Convert to string for matching\n",
    "\n",
    "# Rename other columns\n",
    "df1.rename(columns={\"Total Damage, Adjusted ('000 US$)\": \"Total Damages ('000 US$')\"}, inplace=True)\n",
    "df2.rename(columns={'Dis Mag Value': 'Magnitude', 'Dis Mag Scale': 'Magnitude Scale', \n",
    "                    'No Injured': 'No. Injured', 'No Affected': 'No. Affected', \n",
    "                    'No Homeless': 'No. Homeless'}, inplace=True)\n",
    "\n",
    "# Define merge columns\n",
    "merge_columns = [\n",
    "    \"Year\", \"Disaster Group\", \"Disaster Subgroup\", \"Disaster Type\", \"ISO\",\n",
    "    \"Latitude\", \"Longitude\", \"Start Year\", \"Start Month\", \"Start Day\", \n",
    "    \"End Year\", \"End Month\", \"End Day\", \"Total Deaths\", \"No. Injured\", \n",
    "    \"No. Affected\", \"No. Homeless\", \"Total Affected\", \"Total Damages ('000 US$)\", \n",
    "    \"CPI\", \"Magnitude\", \"Magnitude Scale\"\n",
    "]\n",
    "\n",
    "# Check for missing columns\n",
    "missing_cols_df1 = [col for col in merge_columns if col not in df1.columns]\n",
    "missing_cols_df2 = [col for col in merge_columns if col not in df2.columns]\n",
    "\n",
    "if missing_cols_df1 or missing_cols_df2:\n",
    "    print(\"Missing columns in df1:\", missing_cols_df1)\n",
    "    print(\"Missing columns in df2:\", missing_cols_df2)\n",
    "else:\n",
    "    # Merge DataFrames\n",
    "    merged_df = pd.merge(df1, df2, on=merge_columns, how=\"inner\")\n",
    "\n",
    "    # Drop duplicates and null values\n",
    "    merged_df = merged_df.dropna().drop_duplicates()\n",
    "\n",
    "    # Save the merged data\n",
    "    merged_df.to_csv('Datasets/integrated_data.csv', index=False)\n",
    "    print(\"Merged data successfully saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
